Got it—here are solid, production-friendly ways to trip a “5 errors in 5 minutes” fuse for a Kafka Streams app, with two flavors: (A) keep the app up but stop/resume the stream(s), and (B) shut the whole app down and let your supervisor restart it.

A) Keep the app running; stop/resume the Streams (“pause listener”)
Core idea

Count only the specific error you care about.

If count ≥ 5 within a rolling 5-minute window → stop the KafkaStreams client(s).

Periodically probe (or wait a cooldown) → start them again.

Where to catch the error

Pick the hook that actually sees the error type you care about:

Record deserialization errors → custom DefaultDeserializationExceptionHandler.

Producer send errors → custom DefaultProductionExceptionHandler.

Anything thrown off a stream thread (e.g., your processor/transform) → StreamsUncaughtExceptionHandler + REPLACE_THREAD (for transient) or trigger shutdown logic yourself.

If it’s a business error inside your topology (e.g., transform fails on certain payload), catch in a transform()/process() and route bad records to a DLQ topic; also increment the error counter there. That avoids killing stream threads for data issues.

Minimal sliding-window counter (in-memory)
@Component
public class SlidingErrorCounter {
  private final Deque<Long> timestamps = new ArrayDeque<>();
  private final Duration window = Duration.ofMinutes(5);

  public synchronized void record() {
    long now = System.currentTimeMillis();
    timestamps.addLast(now);
    trim(now);
  }
  public synchronized int countInWindow() {
    long now = System.currentTimeMillis();
    trim(now);
    return timestamps.size();
  }
  private void trim(long now) {
    long cutoff = now - window.toMillis();
    while (!timestamps.isEmpty() && timestamps.peekFirst() < cutoff) {
      timestamps.removeFirst();
    }
  }
}

Wire it into exception handlers
@Configuration
public class StreamsConfig {

  @Bean
  public KafkaStreamsCustomizer streamsCustomizer(
      SlidingErrorCounter counter,
      StreamsController controller // see below
  ) {
    return kafkaStreams -> {
      kafkaStreams.setUncaughtExceptionHandler((t, e) -> {
        if (isTargetError(e)) {
          counter.record();
          if (counter.countInWindow() >= 5) {
            controller.tripBreaker("uncaught");
          }
          return StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.REPLACE_THREAD;
        }
        return StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_APPLICATION;
      });
    };
  }

  private boolean isTargetError(Throwable e) {
    // your matching logic: class, cause chain, message regex, etc.
    return e instanceof YourSpecificException;
  }

  @Bean
  public DeserializationExceptionHandler deserializationHandler(
      SlidingErrorCounter counter, StreamsController controller) {
    return (context, record, exception) -> {
      if (isYourDeserError(exception)) {
        counter.record();
        if (counter.countInWindow() >= 5) controller.tripBreaker("deser");
        // continue OR fail depending on your tolerance for bad data
        return DeserializationHandlerResponse.CONTINUE;
      }
      return DeserializationHandlerResponse.FAIL;
    };
  }

  private boolean isYourDeserError(Exception e) {
    return e.getMessage() != null && e.getMessage().contains("YourPattern");
  }
}

Controller to stop/start streams cleanly

If you use Spring Kafka Streams (StreamsBuilderFactoryBean), you can stop and start it without killing the app:

@Component
public class StreamsController {
  private final StreamsBuilderFactoryBean factoryBean;
  private final AtomicBoolean open = new AtomicBoolean(true);

  public StreamsController(StreamsBuilderFactoryBean factoryBean) {
    this.factoryBean = factoryBean;
  }

  public void tripBreaker(String reason) {
    if (open.compareAndSet(true, false)) {
      // stop consuming; keep app alive
      factoryBean.stop(); // closes KafkaStreams client
      // optionally publish an app event or log an operational alert
      System.out.println("Streams paused by circuit breaker due to: " + reason);
    }
  }

  public boolean isOpen() { return open.get(); }

  public void resetBreakerAndStart() {
    if (open.compareAndSet(false, true)) {
      factoryBean.start();
      System.out.println("Streams resumed by circuit breaker.");
    }
  }
}

Auto-recovery policy (probe or cooldown)

Simple cooldown: wait N minutes, then try start().

Health-based: ping dependencies (schema registry, DB, upstream service), or check DLQ rate trending down, then resume.

Example scheduled task:

@Component
public class StreamsRecovery {
  private final StreamsController controller;

  public StreamsRecovery(StreamsController controller) {
    this.controller = controller;
  }

  @Scheduled(fixedDelay = 60_000)
  public void tryResume() {
    if (!controller.isOpen() && canResume()) {
      controller.resetBreakerAndStart();
    }
  }

  private boolean canResume() {
    // e.g., last 5 minutes < 5 errors, or external health check ok
    return true;
  }
}


Using Micrometer counters + an Alert (Grafana/CloudWatch) is nice: ops sees when the breaker opens/closes.

If you’re on Spring Cloud Stream (Kafka Streams binder)

You can stop a binding instead of the whole app using BindingService / BindingLifecycleController (you’ve used this earlier). Conceptually identical: on threshold, stop("inputBindingName"); on recovery, start("inputBindingName"). This “pauses” consumption while keeping the JVM up.

B) Shut the whole app down (supervisor restarts it)
Core idea

Same sliding-window counter, but when tripped:

Flush/close Streams → factoryBean.stop() (or kafkaStreams.close(Duration)).

Exit JVM with non-zero code → systemd/Kubernetes/docker orchestrator restarts it (back-off).

@Component
public class FatalBreaker {
  private final StreamsBuilderFactoryBean factoryBean;
  public FatalBreaker(StreamsBuilderFactoryBean factoryBean) { this.factoryBean = factoryBean; }

  public void exitApp(String reason) {
    try { factoryBean.stop(); } catch (Exception ignored) {}
    // ensure logs flushed
    System.err.println("Exiting due to breaker: " + reason);
    // Prefer Spring exit so hooks run:
    int code = SpringApplication.exit(
      ApplicationContextProvider.get(), () -> 1);
    System.exit(code);
  }
}


Set your orchestrator with:

RestartPolicy: always.

Backoff: exponential (e.g., 30s → 2m → 5m) to avoid crash-loops during prolonged incidents.

Liveness/Readiness: Streams “RUNNING” gates readiness; breaker flips it to “not ready” before exit.

Design tips & edge cases

Scope: Decide if the threshold is global, per-topic, or per-exception-type. If per-type, keep separate counters (Map<ErrorKey, SlidingErrorCounter>).

Persistence: In-memory is fine for transient control. If you want breaker state to survive restarts, store the last N timestamps in a local state store (RocksDB) or Redis.

What to count: Only your target error. Don’t mix with benign “record too large”, timeouts, etc.

DLQ: Always DLQ the bad record with headers (topic/partition/offset/error) so you can reprocess later.

Visibility: Emit ApplicationEvents when breaker opens/closes; wire to your alerting.

Streams response choice: For uncaught exceptions, REPLACE_THREAD keeps the app healthy if the error is data-driven; combine with breaker to guard floods. Use SHUTDOWN_CLIENT only when you want to force stop.

Quick pick: which approach when?

Data spikes / flaky upstream / transient schema issues → A) Pause Streams, auto-resume after health is green. Users of the app (HTTP, schedulers) remain online.

Corrupt deploy / severe infra outage → B) Exit and let the platform restart with back-off. Keeps your fleet consistent and leverages orchestration.

If you tell me the exact error you want to track (class/message) and whether you’re on Spring Kafka Streams direct vs Spring Cloud Stream binder, I’ll drop in the exact handler + binder start/stop snippets tailored to your binding names.
