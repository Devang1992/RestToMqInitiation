Kafka ItemReader Config
java
Copy
Edit
@Configuration
public class KafkaBatchConfig {

    @Value("${spring.kafka.bootstrap-servers}")
    private String bootstrapServers;

    @Bean
    public KafkaItemReader<String, String> kafkaItemReader() {
        KafkaItemReader<String, String> reader = new KafkaItemReaderBuilder<String, String>()
                .partitions(0)  // or .partitions(0, 1, ...) for multiple
                .consumerProperties(Map.of(
                        ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers,
                        ConsumerConfig.GROUP_ID_CONFIG, "batch-consumer-group",
                        ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest",
                        ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class,
                        ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class
                ))
                .name("kafka-item-reader")
                .topic("your-topic-name")
                .build();

        return reader;
    }
}
✅ 4. Spring Batch Job & Step
java
Copy
Edit
@Configuration
public class BatchJobConfig {

    @Bean
    public Step kafkaStep(StepBuilderFactory stepBuilderFactory,
                          KafkaItemReader<String, String> kafkaItemReader) {
        return stepBuilderFactory.get("kafkaStep")
                .<String, String>chunk(10)
                .reader(kafkaItemReader)
                .writer(items -> {
                    for (String item : items) {
                        System.out.println("Processed message: " + item);
                    }
                })
                .build();
    }

    @Bean
    public Job kafkaBatchJob(JobBuilderFactory jobBuilderFactory, Step kafkaStep) {
        return jobBuilderFactory.get("kafkaBatchJob")
                .start(kafkaStep)
                .build();
    }
}
✅ 5. Trigger from Controller
java
Copy
Edit
@RestController
@RequestMapping("/batch")
public class BatchController {

    private final JobLauncher jobLauncher;
    private final Job kafkaBatchJob;

    public BatchController(JobLauncher jobLauncher, @Qualifier("kafkaBatchJob") Job kafkaBatchJob) {
        this.jobLauncher = jobLauncher;
        this.kafkaBatchJob = kafkaBatchJob;
    }

    @GetMapping("/run")
    public ResponseEntity<String> runKafkaJob() {
        try {
            JobParameters jobParameters = new JobParametersBuilder()
                    .addLong("time", System.currentTimeMillis())
                    .toJobParameters();

            jobLauncher.run(kafkaBatchJob, jobParameters);

            return ResponseEntity.ok("Kafka batch job started");
        } catch (Exception e) {
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body("Error: " + e.getMessage());
        }
    }
}

----------------------------------------

application.yml
yaml
Copy
Edit
app:
  kafka:
    topic: my-topic
    group-id: batch-consumer-group

spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        spring.json.trusted.packages: "*"

  batch:
    job:
      enabled: false
✅ 2. MyEvent POJO (Value)
java
Copy
Edit
@Data
@NoArgsConstructor
@AllArgsConstructor
public class MyEvent {
    private String id;
    private String name;
    private String timestamp;
}
✅ 3. Kafka Properties Config Class
java
Copy
Edit
@Component
@ConfigurationProperties(prefix = "app.kafka")
@Data
public class KafkaBatchProperties {
    private String topic;
    private String groupId;
}
✅ 4. KafkaItemReader Config (Deserialize JSON → POJO)
java
Copy
Edit
@Configuration
public class KafkaBatchReaderConfig {

    @Autowired
    private KafkaBatchProperties props;

    @Autowired
    private KafkaProperties kafkaProperties;

    @Bean
    public KafkaItemReader<String, MyEvent> kafkaItemReader() {
        Map<String, Object> consumerProps = kafkaProperties.buildConsumerProperties(props.getGroupId());
        consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        consumerProps.put(JsonDeserializer.TRUSTED_PACKAGES, "*");

        return new KafkaItemReaderBuilder<String, MyEvent>()
                .name("kafka-item-reader")
                .topic(props.getTopic())
                .partitions(0) // You can extend to multiple partitions
                .consumerProperties(consumerProps)
                .keyDeserializer(new StringDeserializer())
                .valueDeserializer(new JsonDeserializer<>(MyEvent.class, false))
                .build();
    }
}
✅ 5. Spring Batch Job Config
java
Copy
Edit
@Configuration
public class KafkaBatchJobConfig {

    @Bean
    public Step kafkaStep(StepBuilderFactory stepBuilderFactory,
                          KafkaItemReader<String, MyEvent> reader) {
        return stepBuilderFactory.get("kafkaStep")
                .<MyEvent, MyEvent>chunk(5)
                .reader(reader)
                .writer(items -> {
                    for (MyEvent event : items) {
                        System.out.println("Processed event: " + event);
                    }
                })
                .build();
    }

    @Bean
    public Job kafkaBatchJob(JobBuilderFactory jobBuilderFactory, Step kafkaStep) {
        return jobBuilderFactory.get("kafkaBatchJob")
                .start(kafkaStep)
                .build();
    }
}
✅ 6. Trigger Job from Controller
java
Copy
Edit
@RestController
@RequestMapping("/batch")
public class KafkaBatchJobController {

    private final JobLauncher jobLauncher;
    private final Job kafkaBatchJob;

    public KafkaBatchJobController(JobLauncher jobLauncher, @Qualifier("kafkaBatchJob") Job kafkaBatchJob) {
        this.jobLauncher = jobLauncher;
        this.kafkaBatchJob = kafkaBatchJob;
    }

    @GetMapping("/run")
    public ResponseEntity<String> runJob() {
        try {
            JobParameters jobParameters = new JobParametersBuilder()
                    .addLong("time", System.currentTimeMillis())
                    .toJobParameters();

            jobLauncher.run(kafkaBatchJob, jobParameters);
            return ResponseEntity.ok("Batch job triggered.");
        } catch (Exception e) {
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)
                    .body("Batch job failed: " + e.getMessage());
        }
    }
}
✅ Summary

-------------------------------------------------------------------------



KafkaAdminConfig.java
java
Copy
Edit
@Configuration
public class KafkaAdminConfig {
    @Bean
    public AdminClient adminClient(KafkaProperties kafkaProperties) {
        return AdminClient.create(kafkaProperties.buildAdminProperties());
    }

    public List<Integer> getPartitions(String topic, AdminClient adminClient) {
        try {
            var desc = adminClient.describeTopics(List.of(topic))
                .values().get(topic).get();
            return desc.partitions().stream()
                       .map(p -> p.partition()).toList();
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
}
5. KafkaJsonReaderConfig.java
java
Copy
Edit
@Configuration
public class KafkaJsonReaderConfig {

    @Autowired KafkaBatchProperties props;
    @Autowired KafkaProperties kafkaProperties;
    @Autowired AdminClient adminClient;
    @Autowired KafkaAdminConfig kafkaAdminConfig;

    @Bean
    public KafkaItemReader<String, JsonNode> reader() {
        var partitions = kafkaAdminConfig.getPartitions(props.getTopic(), adminClient);
        var map = kafkaProperties.buildConsumerProperties(props.getGroupId(), null);
        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        map.put(JsonDeserializer.TRUSTED_PACKAGES, "*");

        Properties consumerProps = new Properties();
        map.forEach(consumerProps::put);

        return new KafkaItemReaderBuilder<String, JsonNode>()
                .name("kafka-json-reader")
                .topic(props.getTopic())
                .partitions(partitions.stream().mapToInt(i -> i).toArray())
                .consumerProperties(consumerProps)
                .keyDeserializer(new StringDeserializer())
                .valueDeserializer(new JsonDeserializer<>(JsonNode.class, false))
                .build();
    }
}
6. KafkaBatchJobConfig.java
java
Copy
Edit
@Configuration
public class KafkaBatchJobConfig {

    @Bean
    public Job kafkaJob(JobRepository jr, Step kafkaStep) {
        return new JobBuilder("kafka-job", jr).start(kafkaStep).build();
    }

    @Bean
    public Step kafkaStep(JobRepository jr, PlatformTransactionManager tx, 
                          KafkaItemReader<String, JsonNode> reader) {
        return new StepBuilder("kafka-step", jr)
            .<JsonNode, JsonNode>chunk(ChunkOptions.options().itemCount(10))
            .reader(reader)
            .writer(items -> items.forEach(j -> log.info("Key: {}, Value: {}", 
                                 j.getHeaders().get("key"), j.toString())))
            .transactionManager(tx)
            .build();
    }
}
7. BatchController.java
java
Copy
Edit
@RestController
@RequestMapping("/batch")
public class BatchController {
    @Autowired JobLauncher jobLauncher;
    @Autowired @Qualifier("kafkaJob") Job job;

    @GetMapping("/run")
    public ResponseEntity<String> run() throws Exception {
        var params = new JobParametersBuilder()
                .addLong("time", System.currentTimeMillis()).toJobParameters();
        jobLauncher.run(job, params);
        return ResponseEntity.ok("Job launched");
    }
}


----------------------\\\

 Here’s What We’ll Do:
To include Kafka headers in Spring Batch using KafkaItemReader, we’ll:

1. Use ConsumerRecord<String, JsonNode> as the item type
➡️ This gives access to:

Key

Value (JsonNode)

Headers

Partition

Offset

2. Update the reader to deserialize into ConsumerRecord
✅ You’ll need a custom valueDeserializer via properties
✅ But you don’t need to parse headers manually — Kafka gives you that

✅ Updated Reader Example
java
Copy
Edit
@Bean
public KafkaItemReader<String, ConsumerRecord<String, JsonNode>> kafkaItemReader(
        KafkaBatchProperties kafkaBatchProperties,
        KafkaProperties kafkaProperties,
        AdminClient adminClient,
        KafkaAdminConfig kafkaAdminConfig
) {
    List<Integer> partitions = kafkaAdminConfig.getPartitions(kafkaBatchProperties.getTopic(), adminClient);

    Map<String, Object> configMap = kafkaProperties.buildConsumerProperties(kafkaBatchProperties.getGroupId(), null);
    configMap.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
    configMap.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
    configMap.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
    configMap.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
    configMap.put(JsonDeserializer.VALUE_DEFAULT_TYPE, "com.fasterxml.jackson.databind.JsonNode");

    Properties consumerProps = new Properties();
    configMap.forEach(consumerProps::put);

    return new KafkaItemReaderBuilder<String, ConsumerRecord<String, JsonNode>>()
            .name("kafka-header-aware-reader")
            .topic(kafkaBatchProperties.getTopic())
            .partitions(partitions.stream().mapToInt(i -> i).toArray())
            .consumerProperties(consumerProps)
            .build();
}
✅ Updated Step Writer to Access Headers
java
Copy
Edit
@Bean
public Step kafkaStep(JobRepository jobRepository,
                      PlatformTransactionManager transactionManager,
                      KafkaItemReader<String, ConsumerRecord<String, JsonNode>> reader) {
    return new StepBuilder("kafka-step", jobRepository)
            .<ConsumerRecord<String, JsonNode>, ConsumerRecord<String, JsonNode>>chunk(10)
            .reader(reader)
            .writer(records -> {
                for (ConsumerRecord<String, JsonNode> record : records) {
                    String templateId = Optional.ofNullable(record.headers().lastHeader("template-id"))
                                                .map(header -> new String(header.value(), StandardCharsets.UTF_8))
                                                .orElse("default");

                    JsonNode payload = record.value();

                    log.info("Template: {}, Payload: {}", templateId, payload.toPrettyString());

                    // Now you can render your templates based on `templateId` and `payload`
                }
            })
            .transactionManager(transactionManager)
            .build();
}
✅ Summary
Field	Access With
Kafka Key	record.key()
Kafka JSON payload	record.value()
Kafka Headers	record.headers()
Template ID from header	record.headers().lastHeader("template-id")

Would you like a full working zip/project for this updated version? Or integrate with your existing template rendering logic now?








You said:
so for wirter not sure if you still remember

we had the requirement to make a final file of all kafka messages combined with the template choosen from headers and append everything with the total number of records appended


ChatGPT said:
Yes — I remember now! Let me summarize your full requirement and then give you the updated ItemWriter that does exactly that.

🧠 Recap of Your Requirement
You want a Kafka batch job that:

Reads Kafka messages as:

String key

JsonNode value (the message payload)

Kafka headers (e.g., template-id)

For each message:

Extract the template-id from headers

Load a corresponding Handlebars or text template from the resources folder

Apply the JSON payload to the template

Append the rendered output to a file

Final output file:

Should contain all rendered message blocks (i.e., batch output)

Add a final line that says something like:
Total records processed: XX

✅ The Writer You Need: (Handles All This)
java
Copy
Edit
@Bean
public Step kafkaStep(JobRepository jobRepository,
                      PlatformTransactionManager transactionManager,
                      KafkaItemReader<String, ConsumerRecord<String, JsonNode>> reader,
                      TemplateMapper templateMapper // <-- your helper class
) {
    return new StepBuilder("kafka-step", jobRepository)
            .<ConsumerRecord<String, JsonNode>, ConsumerRecord<String, JsonNode>>chunk(1000)
            .reader(reader)
            .writer(new ItemWriter<>() {
                private final Path outputFile = Paths.get("/tmp/final-output.txt");
                private int total = 0;

                @Override
                public void write(List<? extends ConsumerRecord<String, JsonNode>> items) throws Exception {
                    Files.deleteIfExists(outputFile); // Clear previous file if rerunning
                    Files.createDirectories(outputFile.getParent());

                    try (BufferedWriter writer = Files.newBufferedWriter(outputFile, StandardCharsets.UTF_8,
                            StandardOpenOption.CREATE, StandardOpenOption.APPEND)) {

                        for (ConsumerRecord<String, JsonNode> record : items) {
                            String templateId = Optional.ofNullable(record.headers().lastHeader("template-id"))
                                    .map(h -> new String(h.value(), StandardCharsets.UTF_8))
                                    .orElse("default");

                            JsonNode payload = record.value();

                            // Use your existing template mapping logic
                            String rendered = templateMapper.renderTemplate(templateId, payload);

                            writer.write(rendered);
                            writer.newLine();
                            total++;
                        }

                        writer.write("\nTotal records processed: " + total);
                        writer.newLine();
                    }
                }
            })
            .transactionManager(transactionManager)
            .build();
}
✅ Required Utility (TemplateMapper)
If you haven't already, your TemplateMapper would look like this:

java
Copy
Edit
@Component
public class TemplateMapper {

    private final Map<String, Template> templateMap = new HashMap<>();

    public TemplateMapper() throws IOException {
        Handlebars handlebars = new Handlebars();

        try (Stream<Path> paths = Files.walk(Paths.get("src/main/resources/templates"))) {
            paths.filter(Files::isRegularFile)
                .forEach(path -> {
                    try {
                        String templateId = path.getFileName().toString().replace(".hbs", "");
                        templateMap.put(templateId, handlebars.compile(path.toString()));
                    } catch (IOException e) {
                        throw new RuntimeException("Failed to load template: " + path, e);
                    }
                });
        }
    }

    public String renderTemplate(String templateId, JsonNode data) throws IOException {
        Template template = templateMap.getOrDefault(templateId, templateMap.get("default"));
        return template.apply(data);
    }
}
