1. Controller Trigger
java
Copy
Edit
@RestController
@RequestMapping("/batch")
public class BatchController {

    private final BatchService batchService;

    public BatchController(BatchService batchService) {
        this.batchService = batchService;
    }

    @PostMapping("/run")
    public ResponseEntity<String> runBatch() {
        batchService.startBatchProcess();
        return ResponseEntity.ok("Batch process triggered.");
    }
}
üß† 2. Batch Service
java
Copy
Edit
@Service
public class BatchService {

    private final KafkaBatchConsumer kafkaBatchConsumer;

    public BatchService(KafkaBatchConsumer kafkaBatchConsumer) {
        this.kafkaBatchConsumer = kafkaBatchConsumer;
    }

    public void startBatchProcess() {
        List<String> messages = kafkaBatchConsumer.pollMessages();
        FileWriterUtil.writeToFile(messages, "output-batch.txt");
    }
}
üõ∞Ô∏è 3. Kafka Consumer (Manual Pull ‚Äì Not @KafkaListener)
java
Copy
Edit
@Component
public class KafkaBatchConsumer {

    private final Consumer<String, String> consumer;

    public KafkaBatchConsumer() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "batch-consumer");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("your-topic-name"));
    }

    public List<String> pollMessages() {
        List<String> result = new ArrayList<>();
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));
        for (ConsumerRecord<String, String> record : records) {
            result.add(record.value());
        }
        return result;
    }
}
üóÇÔ∏è 4. File Writer Utility
java
Copy
Edit
public class FileWriterUtil {

    public static void writeToFile(List<String> lines, String filename) {
        try {
            Path filePath = Paths.get("batches", filename); // output in /batches
            Files.createDirectories(filePath.getParent());
            Files.write(filePath, lines, StandardCharsets.UTF_8, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING);
        } catch (IOException e) {
            throw new RuntimeException("Failed to write batch file", e);
        }
    }
}
‚öôÔ∏è 5. application.yml (for file config, optional)
yaml
Copy
Edit
batch:
  output-dir: batches
  topic-name: your-topic-name
  file-name-prefix: batch-output-
You can then bind this using @ConfigurationProperties(prefix = "batch") if you want to configure file name/format dynamically.



 Modify pollMessages() to filter by timestamp
Kafka messages have a record.timestamp() field (milliseconds since epoch). You can filter with this.

java
Copy
Edit
public List<String> pollMessagesByTime(Instant fromTime) {
    List<String> result = new ArrayList<>();
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));

    for (ConsumerRecord<String, String> record : records) {
        Instant messageTime = Instant.ofEpochMilli(record.timestamp());
        if (messageTime.isAfter(fromTime)) {
            result.add(record.value());
        }
    }

    System.out.println("Total filtered messages: " + result.size());
    return result;
}
üß† 2. Triggering Batch with Time Filtering
In your BatchService, you can pass a time range:

java
Copy
Edit
public void startBatchProcessWithTimeWindow(Duration duration) {
    Instant fromTime = Instant.now().minus(duration);
    List<String> messages = kafkaBatchConsumer.pollMessagesByTime(fromTime);
    System.out.println("Processed " + messages.size() + " messages after " + fromTime);
    FileWriterUtil.writeToFile(messages, "output-batch.txt");
}
In controller:

java
Copy
Edit
@PostMapping("/run")
public ResponseEntity<String> runBatch(@RequestParam(defaultValue = "PT24H") String duration) {
    batchService.startBatchProcessWithTimeWindow(Duration.parse(duration)); // e.g., PT24H for 24 hours
    return ResponseEntity.ok("Batch process completed.");
}
üß™ Example API Call
http
Copy
Edit
POST /batch/run?duration=PT1H
This processes all messages from the last 1 hour.

üì¶ 3. Print Summary Info
At the end of startBatchProcessWithTimeWindow(), you can log:

java
Copy
Edit
System.out.printf("‚úÖ Batch completed: %,d messages processed.%n", messages.size());
üí° Optional Enhancements
Track min/max timestamps of messages processed.

Group by domain or status using Map<String, List<Message>>.

Return a structured summary like:

json
Copy
Edit
{
  "processedMessages": 10344,
  "fromTime": "2025-06-22T00:00:00Z",
  "toTime": "2025-06-23T00:00:00Z"
}
Would you like help adding:

Header filtering (status = completed)?

Domain grouping logic?

Offset saving for resuming?

Let me know ‚Äî I‚Äôll guide you through the next piece.








You said:
so just a question, how can I know that the last message is consumed and there are no new message to the batch proccess can stop here or esle still continue


ChatGPT said:
Great question ‚Äî this is one of the most critical aspects of building a batch processor on Kafka:

üí≠ "How do I know I've consumed everything, and there's nothing new left ‚Äî so I can stop the batch?"

üß† Kafka Basics Recap
Kafka is a log ‚Äî not a queue. You pull messages, and new messages can arrive at any time. So Kafka does not tell you "this is the last message".

But we can infer it using a few techniques.

‚úÖ 3 Common Strategies to Know When You're "Done"
‚úÖ 1. Poll Until No New Messages in X Time
This is the most common and safe way for a batch job.

java
Copy
Edit
public List<String> pollUntilDone() {
    List<String> allMessages = new ArrayList<>();
    int emptyPolls = 0;

    while (emptyPolls < 3) { // 3 empty polls in a row = stop
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(2));
        
        if (records.isEmpty()) {
            emptyPolls++;
        } else {
            emptyPolls = 0;
            for (ConsumerRecord<String, String> record : records) {
                allMessages.add(record.value());
            }
        }
    }

    System.out.println("‚úÖ Done. Total messages: " + allMessages.size());
    return allMessages;
}
‚è≥ You Control:
Poll time window

Max empty retries

How long you're willing to "wait" before deciding it's done

‚úÖ 2. Check Latest Offsets vs. Current Position
A bit more advanced, but reliable.

java
Copy
Edit
TopicPartition partition = new TopicPartition("your-topic", 0);
consumer.assign(Collections.singleton(partition));
consumer.seekToEnd(Collections.singleton(partition));
long endOffset = consumer.position(partition);

consumer.seekToBeginning(Collections.singleton(partition));
while (consumer.position(partition) < endOffset) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(500));
    for (ConsumerRecord<String, String> record : records) {
        // process...
    }
}
üß† This tells Kafka:
"Give me everything from now until what existed when I started."

This guarantees you don‚Äôt process new messages arriving during the batch.

‚úÖ 3. External Marker or Control Message
Not common for raw logs but used in well-designed pipelines:

A producer sends a "done" message (e.g., "{"type": "END"}")

Your consumer checks for this special message and stops

üõ°Ô∏è Best Practice (Recommended for You)
Use #1 (timeout polling) or #2 (offset range scan) in combination with:

Header filtering (status=completed)

Time-based filters (last 24h)

Controller to trigger batch

‚ö†Ô∏è Gotchas to Avoid
Pitfall	Solution
Infinite polling	Set a max retry or timeout
Consuming new incoming messages during batch	Use offset snapshot at batch start
Not committing offset (if needed)	Use manual commit if re-processing isn't allowed

‚úÖ Final Thought
If your job is manual-triggered and you want to:

Process everything that already exists (but not new data),
‚Üí Offset snapshot (Strategy #2) is ideal.

If it's:

Time-windowed (e.g., last 24h) and async,
‚Üí Strategy #1 (idle timeout) is cleaner.
