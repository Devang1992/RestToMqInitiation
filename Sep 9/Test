package your.pkg.reader;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.*;
import org.mockito.junit.jupiter.MockitoExtension;
import org.springframework.batch.item.ExecutionContext;
import org.springframework.batch.item.ItemStreamException;

import java.time.Duration;
import java.util.*;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.*;

@ExtendWith(MockitoExtension.class)
class BatchKafkaReaderTest {

  @Mock KafkaConsumer<String, String> consumer;

  private Properties props;
  private final String topic = "t";
  private final List<Integer> parts = Arrays.asList(0, 1);
  private final Duration timeout = Duration.ofMillis(10);

  // Build the reader while overriding createConsumer to return our mock
  private BatchKafkaReader reader(long startOffset, int maxEmptyPolls) {
    return new BatchKafkaReader(props, topic, parts, timeout, maxEmptyPolls, startOffset) {
      @Override protected KafkaConsumer<String, String> createConsumer(Properties p) { return consumer; }
    };
  }

  @BeforeEach
  void setUp() {
    props = new Properties();
    // If your open() sets AUTO_OFFSET_RESET when startOffset >= 0, thatâ€™s fine.
    // (If your class does NOT have createConsumer(..), add it:
    //   protected KafkaConsumer<String,String> createConsumer(Properties p){ return new KafkaConsumer<>(p); }
    // and call it from open() instead of `new KafkaConsumer<>(props)`.)
  }

  // ---- helpers -------------------------------------------------------------

  private static ConsumerRecords<String, String> empty() {
    return new ConsumerRecords<>(Collections.emptyMap());
  }

  private static ConsumerRecords<String, String> records(String topic, int partition, long startOffset, String... values) {
    Map<TopicPartition, List<ConsumerRecord<String, String>>> data = new HashMap<>();
    List<ConsumerRecord<String, String>> list = new ArrayList<>();
    long off = startOffset;
    for (String v : values) {
      list.add(new ConsumerRecord<>(topic, partition, off++, null, v));
    }
    data.put(new TopicPartition(topic, partition), list);
    return new ConsumerRecords<>(data);
  }

  private static Set<TopicPartition> tpSet(String topic, int... ps) {
    Set<TopicPartition> s = new HashSet<>();
    for (int p : ps) s.add(new TopicPartition(topic, p));
    return s;
  }

  // ---- tests ---------------------------------------------------------------

  @Test
  void open_withManualOffset_seeksAllAndReturns() {
    BatchKafkaReader r = reader(/*startOffset*/ 42L, /*maxEmptyPolls*/ 3);

    r.open(new ExecutionContext());

    // assigned to both partitions
    verify(consumer).assign(Arrays.asList(new TopicPartition(topic,0), new TopicPartition(topic,1)));
    // sought to manual offset on each
    verify(consumer).seek(new TopicPartition(topic,0), 42L);
    verify(consumer).seek(new TopicPartition(topic,1), 42L);
    // no committed() lookup when manual offset used
    verify(consumer, never()).committed(anySet());
  }

  @Test
  void open_withCommittedOffsets_elseSeekBeginning() {
    BatchKafkaReader r = reader(/*startOffset*/ -1, 3);

    Map<TopicPartition, OffsetAndMetadata> committed = new HashMap<>();
    committed.put(new TopicPartition(topic, 0), new OffsetAndMetadata(7L)); // only p0 has commit
    when(consumer.committed(tpSet(topic,0,1))).thenReturn(committed);

    r.open(new ExecutionContext());

    verify(consumer).seek(new TopicPartition(topic,0), 7L);
    // p1 had no commit -> seekToBeginning(p1)
    @SuppressWarnings("unchecked")
    ArgumentCaptor<Set<TopicPartition>> cap = ArgumentCaptor.forClass(Set.class);
    verify(consumer).seekToBeginning(cap.capture());
    assertTrue(cap.getValue().contains(new TopicPartition(topic,1)));
  }

  @Test
  void open_wrapsExceptionsAsItemStreamException() {
    BatchKafkaReader r = reader(-1, 3);
    when(consumer.committed(anySet())).thenThrow(new RuntimeException("boom"));

    ItemStreamException ex = assertThrows(ItemStreamException.class,
        () -> r.open(new ExecutionContext()));
    assertTrue(ex.getMessage().contains("Failed to open consumer"));
  }

  @Test
  void read_happyPath_tracksOffsets_and_commitSyncCommitsNextOffset() {
    BatchKafkaReader r = reader(-1, 3);
    when(consumer.committed(anySet())).thenReturn(Collections.emptyMap());
    r.open(new ExecutionContext());

    // First poll returns 2 records on p0 (offsets 10,11), then empty
    when(consumer.poll(any())).thenReturn(records(topic, 0, 10, "a", "b"))
                              .thenReturn(empty());

    // 1st read returns "a", updates lastOffsets to 11 (10 + 1)
    var rec1 = r.read();
    assertEquals("a", rec1.value());
    // 2nd read returns "b", updates lastOffsets to 12 (11 + 1)
    var rec2 = r.read();
    assertEquals("b", rec2.value());

    // Commit and verify latest offset stored for p0 is 12
    @SuppressWarnings("unchecked")
    ArgumentCaptor<Map<TopicPartition, OffsetAndMetadata>> cap = ArgumentCaptor.forClass(Map.class);
    r.commitSync();
    verify(consumer).commitSync(cap.capture());
    OffsetAndMetadata om = cap.getValue().get(new TopicPartition(topic, 0));
    assertNotNull(om);
    assertEquals(12L, om.offset());
  }

  @Test
  void read_returnsNull_afterMaxEmptyPolls() {
    BatchKafkaReader r = reader(-1, /*maxEmptyPolls*/ 2);
    when(consumer.committed(anySet())).thenReturn(Collections.emptyMap());
    r.open(new ExecutionContext());

    when(consumer.poll(any())).thenReturn(empty()); // always empty

    assertNull(r.read()); // loops internally until 2 empty polls reached
  }

  @Test
  void commitSync_handlesCommitFailed_and_otherExceptions() {
    BatchKafkaReader r = reader(-1, 3);
    when(consumer.committed(anySet())).thenReturn(Collections.emptyMap());
    r.open(new ExecutionContext());

    // Put one record into buffer so lastOffsets is non-empty
    when(consumer.poll(any())).thenReturn(records(topic, 1, 5, "x"));
    assertEquals("x", r.read().value());

    // 1) CommitFailedException branch
    doThrow(new CommitFailedException()).when(consumer).commitSync(anyMap());
    r.commitSync(); // should swallow and log

    // 2) Generic exception branch
    doThrow(new RuntimeException("oops")).when(consumer).commitSync(anyMap());
    r.commitSync(); // should swallow and log

    // close still tries to close the consumer
    r.close();
    verify(consumer).close();
  }

  @Test
  void close_commits_once_and_closes_even_if_commitThrows() {
    BatchKafkaReader r = reader(-1, 3);
    when(consumer.committed(anySet())).thenReturn(Collections.emptyMap());
    r.open(new ExecutionContext());
    when(consumer.poll(any())).thenReturn(records(topic,0,0,"v"));
    r.read(); // populate lastOffsets

    doThrow(new CommitFailedException()).when(consumer).commitSync(anyMap());
    r.close(); // must not throw
    verify(consumer).close();
  }
}
