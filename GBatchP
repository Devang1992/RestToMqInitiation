High-Level Architecture
csharp
Copy
Edit
[Controller]
     |
     V
[Spring Batch Job] ---------------------+
     |                                  |
     V                                  |
[Kafka Listener (Scoped to Job)]       |
     |                                  |
     V                                  |
[Buffered Message Collector]           |
     |                                  |
     V                                  |
[File Writer Step - Mapped Format] <---+
‚úÖ Let's Start Breaking It Down
We'll walk through:

Project structure

Kafka + Spring Batch config

File format loader

Kafka batch reader with timeout

Writing messages using selected format

üìÅ Project Structure (Overview)
css
Copy
Edit
src/main
 ‚îú‚îÄ‚îÄ java
 ‚îÇ   ‚îî‚îÄ‚îÄ com.example.batchkafka
 ‚îÇ       ‚îú‚îÄ‚îÄ controller
 ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ BatchTriggerController.java
 ‚îÇ       ‚îú‚îÄ‚îÄ batch
 ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ KafkaMessageReader.java
 ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ FileFormatMapper.java
 ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ JobCompletionListener.java
 ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ KafkaBatchJobConfig.java
 ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ MessageProcessor.java
 ‚îÇ       ‚îî‚îÄ‚îÄ model
 ‚îÇ           ‚îî‚îÄ‚îÄ KafkaPayload.java
 ‚îî‚îÄ‚îÄ resources
     ‚îú‚îÄ‚îÄ application.yml
     ‚îî‚îÄ‚îÄ formats
         ‚îú‚îÄ‚îÄ format1.txt
         ‚îú‚îÄ‚îÄ format2.txt
         ‚îú‚îÄ‚îÄ format3.txt
         ‚îú‚îÄ‚îÄ format4.txt
         ‚îî‚îÄ‚îÄ format5.txt
1Ô∏è‚É£ Controller
java
Copy
Edit
@RestController
@RequestMapping("/batch")
public class BatchTriggerController {

    private final JobLauncher jobLauncher;
    private final Job kafkaBatchJob;

    public BatchTriggerController(JobLauncher jobLauncher, @Qualifier("kafkaBatchJob") Job kafkaBatchJob) {
        this.jobLauncher = jobLauncher;
        this.kafkaBatchJob = kafkaBatchJob;
    }

    @PostMapping("/start")
    public ResponseEntity<String> triggerBatchJob() throws Exception {
        JobParameters params = new JobParametersBuilder()
                .addLong("startTime", System.currentTimeMillis())
                .toJobParameters();

        jobLauncher.run(kafkaBatchJob, params);

        return ResponseEntity.ok("Batch job started.");
    }
}
2Ô∏è‚É£ Batch Job Config: KafkaBatchJobConfig.java
java
Copy
Edit
@Configuration
@EnableBatchProcessing
public class KafkaBatchJobConfig {

    @Value("${app.kafka.topic}")
    private String kafkaTopic;

    @Bean
    public Job kafkaBatchJob(JobBuilderFactory jobBuilderFactory,
                              StepBuilderFactory stepBuilderFactory,
                              KafkaMessageReader reader,
                              MessageProcessor processor,
                              JobCompletionListener listener) {

        Step step = stepBuilderFactory.get("kafka-step")
                .<KafkaPayload, String>chunk(100)
                .reader(reader)
                .processor(processor)
                .writer(items -> {
                    // write lines to file here, for now console
                    items.forEach(System.out::println);
                })
                .build();

        return jobBuilderFactory.get("kafkaBatchJob")
                .listener(listener)
                .start(step)
                .build();
    }
}
3Ô∏è‚É£ Kafka Message Reader (KafkaMessageReader.java)
java
Copy
Edit
@Component
@StepScope
public class KafkaMessageReader implements ItemReader<KafkaPayload> {

    private final KafkaConsumer<String, String> consumer;
    private final BlockingQueue<ConsumerRecord<String, String>> buffer = new LinkedBlockingQueue<>();
    private boolean stopped = false;
    private long lastPollTime = System.currentTimeMillis();

    public KafkaMessageReader(@Value("#{jobParameters['startTime']}") String startTime) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "batch-consumer-" + startTime);
        props.put("key.deserializer", StringDeserializer.class.getName());
        props.put("value.deserializer", StringDeserializer.class.getName());
        props.put("auto.offset.reset", "earliest");

        consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singleton("your-kafka-topic"));

        Executors.newSingleThreadExecutor().submit(this::pollLoop);
    }

    private void pollLoop() {
        while (!stopped) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
            if (!records.isEmpty()) {
                lastPollTime = System.currentTimeMillis();
                records.forEach(buffer::offer);
            }

            if (System.currentTimeMillis() - lastPollTime > 2 * 60 * 1000) { // 2 minutes no message
                stopped = true;
                consumer.close();
            }
        }
    }

    @Override
    public KafkaPayload read() {
        if (stopped && buffer.isEmpty()) return null;

        try {
            ConsumerRecord<String, String> record = buffer.poll(5, TimeUnit.SECONDS);
            if (record == null) return null;

            String format = Optional.ofNullable(record.headers().lastHeader("format"))
                    .map(h -> new String(h.value()))
                    .orElse("default");

            return new KafkaPayload(record.value(), format);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            return null;
        }
    }
}
4Ô∏è‚É£ File Format Mapper (MessageProcessor.java + FileFormatMapper.java)
KafkaPayload.java
java
Copy
Edit
public class KafkaPayload {
    private final String json;
    private final String format;

    public KafkaPayload(String json, String format) {
        this.json = json;
        this.format = format;
    }

    // getters...
}
FileFormatMapper.java
java
Copy
Edit
@Component
public class FileFormatMapper {

    private final Map<String, String> templates = new HashMap<>();

    @PostConstruct
    public void loadTemplates() throws IOException {
        for (int i = 1; i <= 5; i++) {
            String formatKey = "format" + i;
            String template = Files.readString(
                Paths.get("src/main/resources/formats/" + formatKey + ".txt")
            );
            templates.put(formatKey, template);
        }
    }

    public String applyTemplate(String jsonStr, String formatKey) {
        String template = templates.getOrDefault(formatKey, templates.get("format1"));
        // parse json and apply mapping using regex or formatter
        // dummy for now:
        return template.replace("{raw}", jsonStr);
    }
}
MessageProcessor.java
java
Copy
Edit
@Component
@StepScope
public class MessageProcessor implements ItemProcessor<KafkaPayload, String> {

    private final FileFormatMapper formatMapper;

    public MessageProcessor(FileFormatMapper formatMapper) {
        this.formatMapper = formatMapper;
    }

    @Override
    public String process(KafkaPayload item) {
        return formatMapper.applyTemplate(item.getJson(), item.getFormat());
    }
}
5Ô∏è‚É£ Job Listener (cleanup hook)
java
Copy
Edit
@Component
public class JobCompletionListener extends JobExecutionListenerSupport {
    @Override
    public void afterJob(JobExecution jobExecution) {
        System.out.println("Batch job completed. File writing done.");
    }
}
‚úÖ File Templates (resources/formats/format1.txt ...)
txt
Copy
Edit
Name: {raw}
Later, you can do more advanced mapping like:

txt
Copy
Edit
Name: {user.name}
Age: {user.age}
Children: {children[*].name}
With JSON path mapping or SpEL.

üîö Final Words
This setup:

‚úÖ Uses Spring Batch to control lifecycle

‚úÖ Listens to Kafka only during job

‚úÖ Stops when idle (no message for 2 mins)

‚úÖ Dynamically formats output file

‚úÖ Triggered cleanly from controller

Goal
Write processed messages to a real .txt file in a folder (e.g. /tmp/kafka-output-<timestamp>.txt)

Render each line using JSONPath placeholders from format templates like:

pgsql
Copy
Edit
Name: {$.user.name}
Age: {$.user.age}
üîß Step-by-Step Implementation
üß© 1. ‚úÖ File Writer: Write Output to File (/tmp/)
Modify the Step Writer in your job config:

java
Copy
Edit
@Bean
public Job kafkaBatchJob(JobBuilderFactory jobBuilderFactory,
                          StepBuilderFactory stepBuilderFactory,
                          KafkaMessageReader reader,
                          MessageProcessor processor,
                          JobCompletionListener listener) {

    Step step = stepBuilderFactory.get("kafka-step")
            .<KafkaPayload, String>chunk(100)
            .reader(reader)
            .processor(processor)
            .writer(new FlatFileItemWriter<String>() {{
                setResource(new FileSystemResource("/tmp/kafka-output-" + System.currentTimeMillis() + ".txt"));
                setLineAggregator(item -> item); // each item is already a line
                afterPropertiesSet();
            }})
            .build();

    return jobBuilderFactory.get("kafkaBatchJob")
            .listener(listener)
            .start(step)
            .build();
}
You can customize /tmp/ path via application.yml later if needed.

üß© 2. ‚úÖ JSONPath-Based Template Rendering
Add Jayway's JSONPath library:

üì¶ In pom.xml:
xml
Copy
Edit
<dependency>
    <groupId>com.jayway.jsonpath</groupId>
    <artifactId>json-path</artifactId>
    <version>2.8.0</version>
</dependency>
üõ† Update FileFormatMapper.java for JSONPath
java
Copy
Edit
import com.jayway.jsonpath.JsonPath;
import net.minidev.json.JSONObject;

@Component
public class FileFormatMapper {

    private final Map<String, List<String>> templateLines = new HashMap<>();

    @PostConstruct
    public void loadTemplates() throws IOException {
        for (int i = 1; i <= 5; i++) {
            String formatKey = "format" + i;
            Path path = Paths.get("src/main/resources/formats/" + formatKey + ".txt");
            List<String> lines = Files.readAllLines(path);
            templateLines.put(formatKey, lines);
        }
    }

    public String applyTemplate(String jsonStr, String formatKey) {
        List<String> lines = templateLines.getOrDefault(formatKey, templateLines.get("format1"));
        Object jsonDoc = JsonPath.parse(jsonStr);

        return lines.stream()
                .map(line -> replacePlaceholders(line, jsonDoc))
                .collect(Collectors.joining("\n"));
    }

    private String replacePlaceholders(String line, Object jsonDoc) {
        Matcher matcher = Pattern.compile("\\{\\$.*?}").matcher(line);
        StringBuffer sb = new StringBuffer();

        while (matcher.find()) {
            String pathExpr = matcher.group();
            String jsonPath = pathExpr.substring(1, pathExpr.length() - 1); // remove {}
            Object result;
            try {
                result = JsonPath.read(jsonDoc, jsonPath);
            } catch (Exception e) {
                result = "null";
            }
            matcher.appendReplacement(sb, Matcher.quoteReplacement(result != null ? result.toString() : ""));
        }
        matcher.appendTail(sb);
        return sb.toString();
    }
}
‚úÖ Template Example (format1.txt)
Located in: src/main/resources/formats/format1.txt

txt
Copy
Edit
Name: {$.user.name}
Age: {$.user.age}
Email: {$.user.email}
---
This supports JSONPath like $.user.name, $.children[0].name, etc.

üì¶ Sample Input JSON
json
Copy
Edit
{
  "user": {
    "name": "Devang",
    "age": 28,
    "email": "devang@example.com"
  }
}
‚úÖ Output to File /tmp/kafka-output-<timestamp>.txt
txt
Copy
Edit
Name: Devang
Age: 28
Email: devang@example.com
---
‚úÖ Summary
Feature	Done
File writer to /tmp/	‚úÖ
JSONPath-based format mapping	‚úÖ
Template selection via Kafka header	‚úÖ
Extendable formats	‚úÖ

üöÄ Optional Next
Would you like to:

Make /tmp/ path configurable?

Support multiple format blocks in one file?

Split files per Kafka header (format)?

Upload output file to cloud (S3, Azure)?

Let me know!








You said:
Make /tmp/ path configurable?, use different file format( which is just like some have name first address second some have name second like that but want to render with json path just flexible to choose baseed on header which format)


ChatGPT said:
Awesome ‚Äî you're asking for two things now:

‚úÖ Goal
Make file output path configurable

Support multiple flexible formats (field order & content controlled via header like format=format2)

Template contains lines with JSONPath placeholders

Messages are rendered according to that template dynamically

‚úÖ Implementation Plan
üß© 1. Make /tmp/ configurable
üîß In application.yml
yaml
Copy
Edit
app:
  output:
    dir: /tmp/kafka-exports
üìÑ Inject it in KafkaBatchJobConfig.java
java
Copy
Edit
@Value("${app.output.dir}")
private String outputDir;

@Bean
public Job kafkaBatchJob(...) {
    String fileName = "kafka-output-" + System.currentTimeMillis() + ".txt";
    String filePath = Paths.get(outputDir, fileName).toString();

    Step step = stepBuilderFactory.get("kafka-step")
            .<KafkaPayload, String>chunk(100)
            .reader(reader)
            .processor(processor)
            .writer(new FlatFileItemWriter<String>() {{
                setResource(new FileSystemResource(filePath));
                setLineAggregator(item -> item); // line is already rendered
                afterPropertiesSet();
            }})
            .build();

    return jobBuilderFactory.get("kafkaBatchJob")
            .listener(listener)
            .start(step)
            .build();
}
üß© 2. Flexible File Format Mapping Using JSONPath (already mostly done)
Ensure your formats/ folder has 5 format templates:

üìÑ src/main/resources/formats/format1.txt
txt
Copy
Edit
Name: {$.user.name}
Age: {$.user.age}
Address: {$.user.address}
---
üìÑ format2.txt
txt
Copy
Edit
Address: {$.user.address}
Name: {$.user.name}
---
üìÑ format3.txt
txt
Copy
Edit
Full Info: {$.user.name}, {$.user.age}, {$.user.email}
---
You can have any structure ‚Äî the only requirement is placeholders are valid JSONPath expressions wrapped in {}

üîÅ Use Header to Pick Format
Already in place via:

java
Copy
Edit
String format = Optional.ofNullable(record.headers().lastHeader("format"))
                        .map(h -> new String(h.value()))
                        .orElse("format1"); // default
The file format mapper uses this to choose the correct template.

üß† Internal Flow:
Kafka message comes in ‚Üí Header: format=format3

FileFormatMapper loads format3.txt

Each line in template is scanned for {$.json.path} patterns

Values from JSON are injected into line

Final string is passed to writer

‚úÖ Example:
Kafka Message
json
Copy
Edit
{
  "user": {
    "name": "Devang",
    "age": 28,
    "address": "Toronto",
    "email": "d@example.com"
  }
}
Header: format: format2
Output File:
txt
Copy
Edit
Address: Toronto
Name: Devang
---

