Ah gotcha—you mean: only start listening when the controller endpoint is hit. Yes, keep that flow. Here’s the clean pattern:

1) Container bean (don’t auto-start)
java
Copy
@Bean
KafkaMessageListenerContainer<String, String> batchContainer(
    ConsumerFactory<String, String> cf, KafkaBatchProperties props,
    BatchJobListener listener) {

  var cp = new ContainerProperties(props.getKafka().getTopic());
  cp.setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);
  var c = new KafkaMessageListenerContainer<>(cf, cp);
  c.setAutoStartup(false);            // <- important
  c.setCommonErrorHandler(new DefaultErrorHandler(new FixedBackOff(0, 0)));
  listener.attach(c);                 // listener implements AcknowledgingConsumerAwareMessageListener
  return c;
}
2) Listener (done check uses position() >= endSnapshot)
java
Copy
@Component
public class BatchJobListener implements AcknowledgingConsumerAwareMessageListener<String,String> {
  private KafkaMessageListenerContainer<String,String> container;
  private Map<TopicPartition,Long> endSnapshot = Map.of();

  void attach(KafkaMessageListenerContainer<String,String> c){ this.container = c; }

  void setSnapshot(Map<TopicPartition,Long> snap){ this.endSnapshot = snap; }

  @Override
  public void onMessage(ConsumerRecord<String,String> rec, Acknowledgment ack, Consumer<?,?> consumer) {
    // process...
    ack.acknowledge();

    boolean done = consumer.assignment().stream()
        .allMatch(tp -> consumer.position(tp) >= endSnapshot.getOrDefault(tp, Long.MAX_VALUE));

    if (done) container.stop(); // finishes the run
  }
}
3) Controller starts listening on demand
Snapshot end offsets before starting (so it’s a bounded batch), then start the container.

java
Copy
@RestController
@RequestMapping("/batch")
public class BatchController {
  private final KafkaMessageListenerContainer<String,String> container;
  private final ConsumerFactory<String,String> cf;
  private final KafkaBatchProperties props;
  private final BatchJobListener listener;

  public BatchController(KafkaMessageListenerContainer<String,String> c,
                         ConsumerFactory<String,String> cf,
                         KafkaBatchProperties props,
                         BatchJobListener listener) {
    this.container = c; this.cf = cf; this.props = props; this.listener = listener;
  }

  @PostMapping("/run")
  public ResponseEntity<?> run() {
    if (container.isRunning()) return ResponseEntity.status(409).body("Already running");

    // Take snapshot & choose start offsets
    try (var tmp = cf.createConsumer()) {
      var parts = tmp.partitionsFor(props.getKafka().getTopic()).stream()
          .map(p -> new TopicPartition(p.topic(), p.partition())).toList();
      tmp.assign(parts);

      // choose where to start
      var begins = tmp.beginningOffsets(parts);
      for (var tp : parts) tmp.seek(tp, begins.get(tp)); // or keep committed

      var endSnap = tmp.endOffsets(parts);               // LEO at start time
      listener.setSnapshot(endSnap);
    }

    container.start();                                   // <- starts listening now
    return ResponseEntity.accepted().body("Started");
  }
}
---------------------------------------------------------------

Container config (manual ack + not auto-start)
java
Copy
Edit
@Bean
KafkaMessageListenerContainer<Stri--- String> batchContainer(
    ConsumerFactory<String, String> cf, KafkaBatchProperties props, BatchJobListener listener) {

  ContainerProperties cp = new ContainerProperties(props.getKafka().getTopic());
  cp.setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);

  KafkaMessageListenerContainer<String, String> c =
      new KafkaMessageListenerContainer<>(cf, cp);
  c.setAutoStartup(false);
  c.setCommonErrorHandler(new DefaultErrorHandler(new FixedBackOff(0L, 0L)));

  listener.attach(c);
  return c;
}
Listener: snapshot endOffsets at start, then compare positions
java
Copy
Edit
@Component
public class BatchJobListener implements AcknowledgingConsumerAwareMessageListener<String, String> {

  private final TemplateMapper mapper;
  private final BatchFileGenerator fileGen;
  private KafkaMessageListenerContainer<String, String> container;

  private Map<TopicPartition, Long> endSnapshot = Map.of();
  private final List<String> rendered = Collections.synchronizedList(new ArrayList<>());

  public BatchJobListener(TemplateMapper mapper, BatchFileGenerator fileGen) {
    this.mapper = mapper; this.fileGen = fileGen;
  }

  void attach(KafkaMessageListenerContainer<String, String> c) {
    this.container = c;
    // snapshot when partitions are assigned
    c.getContainerProperties().setConsumerRebalanceListener(new ConsumerAwareRebalanceListener() {
      @Override public void onPartitionsAssigned(Consumer<?, ?> consumer, Collection<TopicPartition> parts) {
        // choose your start (beginning or committed)
        Map<TopicPartition, Long> begins = consumer.beginningOffsets(parts);
        for (TopicPartition tp : parts) consumer.seek(tp, begins.get(tp));

        endSnapshot = consumer.endOffsets(parts);  // LEO at start time
      }
    });
  }

  @Override
  public void onMessage(ConsumerRecord<String, String> rec, Acknowledgment ack, Consumer<?, ?> consumer) {
    rendered.add(mapper.mapTemplate(rec.value(), "NA", "NA"));
    ack.acknowledge();

    // done when position has reached snapshot end on all assigned partitions
    boolean done = true;
    for (TopicPartition tp : consumer.assignment()) {
      long pos = consumer.position(tp);                 // next offset to fetch (advances across skipped transactional stuff)
      long end = endSnapshot.getOrDefault(tp, Long.MAX_VALUE);
      if (pos < end) { done = false; break; }
    }

    if (done) {
      try { fileGen.generateFile(new ArrayList<>(rendered), /*output*/ Path.of("...")); }
      catch (Exception e) { /* log */ }
      finally { container.stop(); }
    }
  }
}
Why this works with transactions
Control/transaction markers still increment offsets, but you don’t see them as records.

The consumer position advances across those markers and any aborted data.

Once the position reaches your snapshot LEO, you have fully “caught up” to the moment you started—regardless of how many offsets were invisible to read_committed.

One nuance (open transactions at snapshot time)
If a transaction is open at snapshot time, endSnapshot may include offsets you can’t pass until that transaction is committed/aborted and the LSO advances. In practice, the consumer position will stop short until commit, then jump—your job will finish once it becomes readable. That’s usually desirable for a bounded, consistent batch.

If you prefer a hard timeout instead, add an idle timer (e.g., stop if no new records and positions don’t move for N seconds).



---------------------------------------------------------------------------

Option 2 — Use a Spring Kafka listener container you can start/stop on demand
This gives you the “start listening on controller call; stop when done” behavior safely.

java
Copy
Edit
// config
@Bean
public KafkaMessageListenerContainer<String, JsonNode> batchContainer(
    ConsumerFactory<String, JsonNode> cf, KafkaBatchProperties props) {

  // assign partitions explicitly; don’t auto-start
  var topic = props.getKafka().getTopic();
  // you can discover partitions dynamically too
  ContainerProperties cp = new ContainerProperties(new TopicPartitionInitialOffset(topic, 0));
  cp.setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);
  cp.setAutoStartup(false);

  return new KafkaMessageListenerContainer<>(cf, cp);
}
Job-scoped listener that tracks end offsets and stops itself:

java
Copy
Edit
@Component
public class BatchJobListener implements AcknowledgingMessageListener<String, JsonNode> {
  private final TemplateMapper mapper;
  private final BatchFileGenerator fileGen;
  private final KafkaMessageListenerContainer<String, JsonNode> container;
  private final ConsumerFactory<String, JsonNode> cf;
  private final KafkaBatchProperties props;

  private volatile Map<TopicPartition, Long> endSnapshot = Map.of();
  private final Map<TopicPartition, Long> last = new ConcurrentHashMap<>();
  private final List<String> rendered = Collections.synchronizedList(new ArrayList<>());

  public BatchJobListener(TemplateMapper mapper,
                          BatchFileGenerator fileGen,
                          KafkaMessageListenerContainer<String, JsonNode> container,
                          ConsumerFactory<String, JsonNode> cf,
                          KafkaBatchProperties props) {
    this.mapper = mapper; this.fileGen = fileGen; this.container = container;
    this.cf = cf; this.props = props;
    container.setupMessageListener(this);
  }

  public void startRun() {
    // snapshot end before starting
    try (var c = cf.createConsumer()) {
      var parts = c.partitionsFor(props.getKafka().getTopic()).stream()
          .map(p -> new TopicPartition(p.topic(), p.partition())).toList();
      c.assign(parts);
      endSnapshot = c.endOffsets(parts);
      last.clear();
      rendered.clear();
      container.start();
    }
  }

  @Override
  public void onMessage(ConsumerRecord<String, JsonNode> rec, Acknowledgment ack) {
    rendered.add(mapper.mapTemplate(rec.value().toString(), "NA", "NA"));
    last.put(new TopicPartition(rec.topic(), rec.partition()), rec.offset());
    ack.acknowledge();

    boolean done = endSnapshot.entrySet().stream()
        .allMatch(e -> last.getOrDefault(e.getKey(), -1L) >= e.getValue() - 1);

    if (done) {
      try { fileGen.generateFile(new ArrayList<>(rendered), props.outputPath()); }
      catch (IOException e) { /* log */ }
      finally { container.stop(); }
    }
  }
}
Controller:

java
Copy
Edit
@RestController
@RequestMapping("/batch")
public class BatchController {
  private final BatchJobListener listener;
  public BatchController(BatchJobListener listener) { this.listener = listener; }

  @PostMapping("/run")
  public ResponseEntity<?> run() {
    listener.startRun();
    return ResponseEntity.accepted().build();
  }
}
This approach is great if you really want a listener that you can start and stop per request. It avoids sharing a raw KafkaConsumer.
