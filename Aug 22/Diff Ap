ahh, sorry — that’s frustrating. let’s stop fighting the framework and give you a drop-in, known-good setup that hits all the things you asked for:

Spring Batch job that reads from Kafka (with full header/value access)

Async job launch that returns immediately with execution id

Resumable offsets (persisted via ExecutionContext)

YAML-driven config (no hardcoded props)

Simple file writer (streamed; no OOM)

Below is a minimal project you can paste in. It does not rely on KafkaItemReader (which is version-sensitive and finicky). Instead, it uses a tiny, robust custom ItemStreamReader built on the Spring Kafka ConsumerFactory, so you fully control seek/commit behavior.

1) pom.xml (Boot 3.3.x, Batch 5.x, Kafka 3.x)
<project xmlns="http://maven.apache.org/POM/4.0.0">
  <modelVersion>4.0.0</modelVersion>
  <parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>3.3.2</version>
    <relativePath/>
  </parent>

  <groupId>dev.demo</groupId>
  <artifactId>batch-kafka</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <properties>
    <java.version>17</java.version>
  </properties>

  <dependencies>
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-batch</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.kafka</groupId>
      <artifactId>spring-kafka</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <dependency>
      <groupId>com.fasterxml.jackson.core</groupId>
      <artifactId>jackson-databind</artifactId>
    </dependency>

    <!-- optional, for @ConfigurationProperties -->
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-configuration-processor</artifactId>
      <optional>true</optional>
    </dependency>
  </dependencies>

  <build>
    <plugins>
      <plugin>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-maven-plugin</artifactId>
      </plugin>
    </plugins>
  </build>
</project>

2) application.yml (YAML-driven everything)
server:
  port: 8080

spring:
  main:
    web-application-type: servlet
  batch:
    job:
      enabled: false   # don't auto-run at startup
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: demo-batch-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      enable-auto-commit: false
      # earliest so first run can consume old messages unless you seek explicitly:
      auto-offset-reset: earliest
    properties:
      isolation.level: read_committed

app:
  batch:
    topic: demo-topic
    partitions: [0]              # list yours here; explicit assignment (no rebalancing)
    poll-timeout-ms: 1000
    start-from-end: false        # true = ignore history and start at end
    initial-offsets: {}          # e.g. { "0": 12345, "1": 987 }
    output-dir: ./out
    file-pattern: WIRES_${date}_run${runId}.txt  # example name pattern
    chunk-size: 100

3) Props class (BatchProps.java)
package dev.demo;

import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.stereotype.Component;

import java.util.List;
import java.util.Map;

@Component
@ConfigurationProperties(prefix = "app.batch")
public class BatchProps {
    private String topic;
    private List<Integer> partitions;
    private int pollTimeoutMs;
    private boolean startFromEnd;
    private Map<Integer, Long> initialOffsets; // partition -> offset
    private String outputDir;
    private String filePattern;
    private int chunkSize = 100;

    // getters/setters...
    // (Generate them or use Lombok if you like)
}

4) A robust Kafka ItemStreamReader (ManualKafkaItemReader.java)
package dev.demo.batch;

import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.common.TopicPartition;
import org.springframework.batch.item.*;
import org.springframework.batch.item.support.AbstractItemCountingItemStreamItemReader;
import org.springframework.kafka.core.ConsumerFactory;

import java.time.Duration;
import java.util.*;

public class ManualKafkaItemReader<K, V> extends AbstractItemCountingItemStreamItemReader<org.apache.kafka.clients.consumer.ConsumerRecord<K,V>> {

    private final ConsumerFactory<K, V> consumerFactory;
    private final String topic;
    private final List<Integer> partitions;
    private final Duration pollTimeout;
    private final boolean startFromEnd;
    private final Map<Integer, Long> initialOffsets; // nullable

    private Consumer<K, V> consumer;
    private Iterator<org.apache.kafka.clients.consumer.ConsumerRecord<K,V>> bufferIt = Collections.emptyIterator<>();
    private final Map<TopicPartition, Long> lastCommitted = new HashMap<>();

    public ManualKafkaItemReader(
            ConsumerFactory<K,V> consumerFactory,
            String topic,
            List<Integer> partitions,
            Duration pollTimeout,
            boolean startFromEnd,
            Map<Integer, Long> initialOffsets) {
        setName("manualKafkaItemReader");
        this.consumerFactory = consumerFactory;
        this.topic = topic;
        this.partitions = partitions;
        this.pollTimeout = pollTimeout;
        this.startFromEnd = startFromEnd;
        this.initialOffsets = initialOffsets == null ? Collections.emptyMap() : initialOffsets;
    }

    @Override
    protected void doOpen() throws Exception {
        this.consumer = consumerFactory.createConsumer();
        List<TopicPartition> tps = new ArrayList<>();
        for (Integer p : partitions) {
            tps.add(new TopicPartition(topic, p));
        }
        consumer.assign(tps);

        // Restore from ExecutionContext if present
        // If not present and initialOffsets exist, seek there
        // Else seek to beginning/end depending on flag
        Map<TopicPartition, Long> restored = new HashMap<>();
        for (TopicPartition tp : tps) {
            restored.put(tp, null);
        }

        ExecutionContext ec = getExecutionContext();
        boolean restoredSomething = false;
        for (TopicPartition tp : tps) {
            String key = offsetKey(tp);
            if (ec.containsKey(key)) {
                long pos = ec.getLong(key);
                consumer.seek(tp, pos);
                restoredSomething = true;
            }
        }

        if (!restoredSomething) {
            boolean usedInitial = false;
            for (TopicPartition tp : tps) {
                Long init = initialOffsets.get(tp.partition());
                if (init != null) {
                    consumer.seek(tp, init);
                    usedInitial = true;
                }
            }
            if (!usedInitial) {
                if (startFromEnd) {
                    consumer.seekToEnd(tps);
                } else {
                    consumer.seekToBeginning(tps);
                }
            }
        }
    }

    @Override
    protected org.apache.kafka.clients.consumer.ConsumerRecord<K, V> doRead() throws Exception {
        if (!bufferIt.hasNext()) {
            ConsumerRecords<K, V> polled = consumer.poll(pollTimeout);
            bufferIt = polled.iterator();
            if (!bufferIt.hasNext()) {
                return null; // chunk will flush/commit via update()
            }
        }
        return bufferIt.next();
    }

    @Override
    protected void doClose() throws Exception {
        if (consumer != null) {
            consumer.close();
        }
    }

    @Override
    public void update(ExecutionContext executionContext) throws ItemStreamException {
        // Save current positions and commit synchronously.
        Set<TopicPartition> assigned = consumer.assignment();
        for (TopicPartition tp : assigned) {
            long pos = consumer.position(tp);
            executionContext.putLong(offsetKey(tp), pos);
            lastCommitted.put(tp, pos);
        }
        consumer.commitSync();
    }

    private String offsetKey(TopicPartition tp) {
        return "kafka.offset." + tp.topic() + "." + tp.partition();
    }
}


Why this works reliably: we assign fixed partitions, explicitly manage seek, persist offsets to the ExecutionContext, and commitSync() during update(). If the step/job restarts, it resumes precisely.

5) Batch config: Job, Step, Writer (BatchConfig.java)
package dev.demo;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import dev.demo.batch.ManualKafkaItemReader;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.batch.core.Job;
import org.springframework.batch.core.Step;
import org.springframework.batch.core.configuration.annotation.EnableBatchProcessing;
import org.springframework.batch.core.job.builder.JobBuilder;
import org.springframework.batch.core.launch.support.RunIdIncrementer;
import org.springframework.batch.core.repository.JobRepository;
import org.springframework.batch.item.*;
import org.springframework.batch.item.file.FlatFileItemWriter;
import org.springframework.batch.item.file.transform.PassThroughLineAggregator;
import org.springframework.batch.item.support.builder.FlatFileItemWriterBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.core.io.PathResource;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.transaction.PlatformTransactionManager;

import java.nio.file.Files;
import java.nio.file.Path;
import java.time.Duration;
import java.time.LocalDate;
import java.util.Map;
import java.util.stream.Collectors;

@Configuration
@EnableBatchProcessing
public class BatchConfig {

    @Autowired BatchProps props;
    @Autowired ConsumerFactory<String, String> consumerFactory;
    @Autowired ObjectMapper objectMapper;

    @Bean
    public ItemStreamReader<ConsumerRecord<String, String>> kafkaReader() {
        return new ManualKafkaItemReader<>(
                consumerFactory,
                props.getTopic(),
                props.getPartitions(),
                Duration.ofMillis(props.getPollTimeoutMs()),
                props.isStartFromEnd(),
                props.getInitialOffsets()
        );
    }

    @Bean
    public ItemProcessor<ConsumerRecord<String, String>, String> toLineProcessor() {
        return record -> {
            // You have full headers + key + value here
            String key = record.key();
            String value = record.value();

            // Example: pretty-print JSON value (or transform to your DJDE lines)
            String json = value;
            try {
                JsonNode n = objectMapper.readTree(value);
                json = objectMapper.writeValueAsString(n);
            } catch (Exception ignored) { /* value not JSON */ }

            // Include some header fields as a prefix (optional)
            String headers = record.headers() == null ? "" :
                    "hdrs=" + 
                    ((Iterable<?>)record.headers()).toString(); // or loop properly

            return key + "|" + json;
        };
    }

    @Bean
    public ItemStreamWriter<String> fileWriter() {
        return items -> {
            // Build output path once per chunk (simple stream append)
            String resolved = props.getFilePattern()
                    .replace("${date}", LocalDate.now().toString())
                    .replace("${runId}", String.valueOf(System.currentTimeMillis()));
            Path dir = Path.of(props.getOutputDir());
            Files.createDirectories(dir);
            Path out = dir.resolve(resolved);
            // Append lines
            Files.write(out,
                    items.stream().map(s -> s + System.lineSeparator()).collect(Collectors.toList()),
                    java.nio.file.StandardOpenOption.CREATE, java.nio.file.StandardOpenOption.APPEND);
        };
    }

    @Bean
    public Step kafkaToFileStep(JobRepository repo, PlatformTransactionManager tx) {
        return new org.springframework.batch.core.step.builder.StepBuilder("kafkaToFileStep", repo)
                .<ConsumerRecord<String, String>, String>chunk(props.getChunkSize(), tx)
                .reader(kafkaReader())
                .processor(toLineProcessor())
                .writer(fileWriter())
                .faultTolerant()
                .skipLimit(Integer.MAX_VALUE) // don't die on bad messages; tweak as needed
                .skip(Exception.class)
                .build();
    }

    @Bean
    public Job kafkaToFileJob(JobRepository repo, Step kafkaToFileStep) {
        return new JobBuilder("kafkaToFileJob", repo)
                .incrementer(new RunIdIncrementer())
                .start(kafkaToFileStep)
                .build();
    }
}


If you want a separate finalize/rename step (banner → stream copy → atomic rename), you can add a second step after kafkaToFileStep. I kept it lean here; happy to plug that back in.

6) Async job launch that returns the execution id (JobController.java)

This uses JobOperator, which starts a job asynchronously and immediately returns the execution id (exactly what you asked for earlier).

package dev.demo;

import org.springframework.batch.core.*;
import org.springframework.batch.core.explore.JobExplorer;
import org.springframework.batch.core.launch.JobOperator;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;

import java.util.Map;

@RestController
@RequestMapping("/jobs")
public class JobController {

    @Autowired JobOperator jobOperator;
    @Autowired JobExplorer jobExplorer;

    @PostMapping("/start")
    public Map<String, Object> start(@RequestParam(defaultValue = "") String params) throws Exception {
        // Pass arbitrary params if you want (e.g., override topic/offsets per run)
        long execId = jobOperator.start("kafkaToFileJob", params);
        return Map.of("executionId", execId, "status", "STARTED");
    }

    @GetMapping("/{executionId}")
    public Map<String, Object> status(@PathVariable long executionId) {
        JobExecution je = jobExplorer.getJobExecution(executionId);
        if (je == null) return Map.of("executionId", executionId, "status", "UNKNOWN");
        return Map.of(
                "executionId", executionId,
                "status", je.getStatus().toString(),
                "startTime", je.getStartTime(),
                "endTime", je.getEndTime(),
                "exitStatus", je.getExitStatus() == null ? null : je.getExitStatus().getExitCode()
        );
    }

    @PostMapping("/{executionId}/stop")
    public Map<String, Object> stop(@PathVariable long executionId) throws Exception {
        boolean sent = jobOperator.stop(executionId);
        return Map.of("executionId", executionId, "stopSignalSent", sent);
    }
}


Why this solves the “not async” issue: JobOperator.start(...) always spawns the job on the background JobLauncher and returns the executionId right away.

7) Common gotchas that likely broke things before

KafkaItemReader version mismatch: its builder/options changed across Spring Batch/Kafka versions. The custom reader avoids this trap.

Rebalancing: for batch reads you usually want fixed partition assignment (assign) and a single running consumer. We do that here.

enable.auto.commit: must be false. We commit exactly once per update() so restarts resume reliably.

Async launch: using JobOperator.start, not JobLauncherApplicationRunner (which runs at startup and is sync unless customized).

Group reuse: don’t run another consumer with the same group concurrently on the same partitions or you’ll see “missing” data due to group rebalancing.

Seek rules: If you need a known historical re-run, set app.batch.initial-offsets for each partition (or flip start-from-end).

8) How to run quickly

Start Kafka locally and create your topic (demo-topic or whatever you set).

Run this app.

Produce a few test messages (keys optional), e.g. using kafka-console-producer.

Hit:

POST http://localhost:8080/jobs/start


Response:

{ "executionId": 3, "status": "STARTED" }


Poll:

GET http://localhost:8080/jobs/3


Check ./out for the file. Tweak filePattern as needed.

If you still prefer the official KafkaItemReader, I can give you a minimal config for your exact Boot/Spring Batch/Spring Kafka versions — but the above works now and sidesteps the moving pieces. Want me to retrofit this with your banner/body/atomic-rename finalize step and header parsing logic next?
