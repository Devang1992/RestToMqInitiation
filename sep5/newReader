package com.td.payments.pants.mailservice.config;

import lombok.extern.slf4j.Slf4j;
import org.jetbrains.annotations.NotNull;
import org.springframework.batch.item.ExecutionContext;
import org.springframework.batch.item.ItemStreamException;
import org.springframework.batch.item.ItemStreamReader;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;

import java.time.Duration;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;
import java.util.stream.Collectors;

/**
 * BatchKafkaReader
 *
 * Behavior:
 *  - If manualOffset >= 0: seek every assigned partition to that exact offset.
 *  - Else: seek to committed offset if present; otherwise to beginning.
 *  - read(): drain buffer; otherwise poll once with pollTimeout (e.g., 40s).
 *      • If data arrives within the window -> return first record immediately.
 *      • If no data within the window -> return null (signals end-of-input).
 *  - close(): best-effort commit of lastOffsets; never throws.
 *
 * Notes:
 *  - Configure the underlying consumer with enable.auto.commit=false.
 *  - Spring Batch will repeatedly call read() until it returns null.
 *  - Using a single long poll means the "quiet window" resets on each message.
 */
@Slf4j
public class BatchKafkaReader implements ItemStreamReader<ConsumerRecord<String, String>> {

    private final Properties props;
    private final String topic;
    private final List<Integer> partitions;             // optional; null/empty => discover all
    private final Duration pollTimeout;                 // e.g., Duration.ofSeconds(40)
    private final long manualOffset;                    // >=0 => seek to this; <0 => committed/earliest

    private KafkaConsumer<String, String> consumer;
    private Iterator<ConsumerRecord<String, String>> buffer = Collections.emptyIterator();

    // Next offset to commit per partition
    private final Map<TopicPartition, OffsetAndMetadata> lastOffsets = new HashMap<>();

    // Observability: last offset polled per partition and highest overall
    private final ConcurrentMap<TopicPartition, Long> lastPolledOffsets = new ConcurrentHashMap<>();
    private final AtomicLong highestOffsetSeen = new AtomicLong(-1L);

    private final AtomicBoolean opened = new AtomicBoolean(false);

    public BatchKafkaReader(
            @NotNull Properties props,
            @NotNull String topic,
            List<Integer> partitions,
            @NotNull Duration pollTimeout,
            long manualOffset
    ) {
        this.props = Objects.requireNonNull(props, "props");
        this.topic = Objects.requireNonNull(topic, "topic");
        this.partitions = (partitions == null || partitions.isEmpty()) ? null : List.copyOf(partitions);
        this.pollTimeout = Objects.requireNonNull(pollTimeout, "pollTimeout");
        this.manualOffset = manualOffset;
    }

    @Override
    public void open(@NotNull ExecutionContext context) throws ItemStreamException {
        if (!opened.compareAndSet(false, true)) {
            log.debug("BatchKafkaReader already opened; ignoring subsequent open()");
            return;
        }

        try {
            consumer = new KafkaConsumer<>(props);
            final List<TopicPartition> tps = resolveTopicPartitions(consumer);
            consumer.assign(tps);

            if (manualOffset >= 0) {
                for (TopicPartition tp : tps) {
                    log.info("Seeking {} to manual offset {}", tp, manualOffset);
                    consumer.seek(tp, manualOffset);
                }
                return;
            }

            Map<TopicPartition, OffsetAndMetadata> committed =
                    Optional.ofNullable(consumer.committed(new HashSet<>(tps)))
                            .orElse(Collections.emptyMap());

            Set<TopicPartition> needEarliest = new HashSet<>(tps);
            for (TopicPartition tp : tps) {
                OffsetAndMetadata om = committed.get(tp);
                if (om != null && om.offset() >= 0) {
                    log.info("Seeking {} to committed offset {}", tp, om.offset());
                    consumer.seek(tp, om.offset());
                    needEarliest.remove(tp);
                }
            }
            if (!needEarliest.isEmpty()) {
                log.info("Seeking to beginning for uncommitted partitions {}", needEarliest);
                consumer.seekToBeginning(needEarliest);
            }
        } catch (Exception e) {
            safeCloseConsumer();
            opened.set(false);
            throw new ItemStreamException("Failed to open Kafka consumer", e);
        }
    }

    @Override
    public ConsumerRecord<String, String> read() {
        if (consumer == null) {
            log.debug("read() called but consumer is null (closed/not opened). Returning end-of-input.");
            return null;
        }

        // 1) Drain any buffered records first (no waiting)
        if (buffer.hasNext()) {
            ConsumerRecord<String, String> rec = buffer.next();
            trackForCommit(rec);
            return rec;
        }

        // 2) Poll ONCE up to pollTimeout (e.g., 40s). If nothing arrives in that window, we're done.
        ConsumerRecords<String, String> polled;
        try {
            polled = consumer.poll(pollTimeout);
        } catch (WakeupException we) {
            // Typical during shutdown; do not spam errors.
            log.warn("Consumer wakeup during poll; treating as end-of-input.", we);
            return null;
        } catch (Exception e) {
            // Hard poll failure -> end the stream; the step/controller decides next.
            log.error("Poll failed; ending input. Consumer metrics: {}", consumerSafeMetrics(), e);
            return null;
        }

        if (polled.isEmpty()) {
            // No data within the entire quiet window (e.g., 40s) -> end-of-input
            log.info("No data within ~{} seconds; ending input. lastPolledOffsets={}, highestOffsetSeen={}",
                    Math.max(1, pollTimeout.getSeconds()), lastPolledOffsets, highestOffsetSeen.get());
            return null;
        }

        // 3) We have data: update observability and return first record immediately.
        updateOffsetsObserved(polled);
        buffer = polled.iterator();

        ConsumerRecord<String, String> rec = buffer.next();
        trackForCommit(rec);
        return rec;
    }

    @Override
    public void update(@NotNull ExecutionContext ctx) {
        // No-op: let the step/driver decide commit cadence by calling commitSync() at chunk boundaries.
    }

    /**
     * Best-effort synchronous commit of offsets tracked so far.
     * Safe to call multiple times; becomes a no-op if nothing to commit.
     */
    public void commitSync() {
        if (consumer == null || lastOffsets.isEmpty()) return;

        try {
            consumer.commitSync(new HashMap<>(lastOffsets));
            log.debug("Committed offsets: {}", lastOffsets);
        } catch (CommitFailedException cfe) {
            // Rebalances or concurrent commits can cause this; warn and move on.
            log.warn("Commit failed (likely rebalance). Will retry on next commit/close. Details: {}", cfe.toString());
        } catch (Exception e) {
            log.warn("Unexpected exception during commitSync; offsets may be uncommitted. Details: {}", e.toString());
        }
    }

    @Override
    public void close() {
        try {
            commitSync(); // best-effort; never throw
        } catch (Exception e) {
            log.debug("Suppressing commit exception during close: {}", e.toString());
        } finally {
            safeCloseConsumer();
            opened.set(false);
        }
    }

    /* ---------------------- helpers ---------------------- */

    private List<TopicPartition> resolveTopicPartitions(KafkaConsumer<String, String> c) {
        if (this.partitions != null) {
            return this.partitions.stream()
                    .map(p -> new TopicPartition(topic, p))
                    .collect(Collectors.toList());
        }
        List<PartitionInfo> infos = c.partitionsFor(topic);
        if (infos == null || infos.isEmpty()) {
            throw new IllegalStateException("No partitions found for topic: " + topic);
        }
        List<TopicPartition> tps = infos.stream()
                .map(pi -> new TopicPartition(pi.topic(), pi.partition()))
                .collect(Collectors.toList());
        log.info("Auto-discovered partitions for {} -> {}", topic, tps);
        return tps;
    }

    private void trackForCommit(ConsumerRecord<String, String> rec) {
        lastOffsets.put(
                new TopicPartition(rec.topic(), rec.partition()),
                new OffsetAndMetadata(rec.offset() + 1) // commit the "next" offset
        );
    }

    private void updateOffsetsObserved(ConsumerRecords<String, String> polled) {
        for (TopicPartition tp : polled.partitions()) {
            List<ConsumerRecord<String, String>> recs = polled.records(tp);
            if (!recs.isEmpty()) {
                long lastOffsetInBatch = recs.get(recs.size() - 1).offset();
                lastPolledOffsets.put(tp, lastOffsetInBatch);
                highestOffsetSeen.accumulateAndGet(lastOffsetInBatch, Math::max);
            }
        }
    }

    private String consumerSafeMetrics() {
        try {
            return String.valueOf(consumer.metrics());
        } catch (Exception ignore) {
            return "<unavailable>";
        }
    }

    private void safeCloseConsumer() {
        if (consumer == null) return;
        try {
            consumer.close();
        } catch (Exception e) {
            log.warn("Failed to close consumer cleanly: {}", e.toString());
        } finally {
            consumer = null;
        }
    }

    /* -------- Optional getters for tests/logging -------- */

    public Map<TopicPartition, OffsetAndMetadata> getLastOffsetsSnapshot() {
        return new HashMap<>(lastOffsets);
    }

    public Map<TopicPartition, Long> getLastPolledOffsetsSnapshot() {
        return new HashMap<>(lastPolledOffsets);
    }

    public long getHighestOffsetSeen() {
        return highestOffsetSeen.get();
    }
}
